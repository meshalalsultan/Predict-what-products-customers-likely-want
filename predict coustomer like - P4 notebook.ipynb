{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "################################################################################\n#Licensed Materials - Property of IBM\n#(C) Copyright IBM Corp. 2020\n#US Government Users Restricted Rights - Use, duplication disclosure restricted\n#by GSA ADP Schedule Contract with IBM Corp.\n################################################################################\n\nThe auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms), such agreements located in the link below.\nSpecifically, the Source Components and Sample Materials clause included in the License Information document for\nWatson Studio Auto-generated Notebook applies to the auto-generated notebooks. \nBy downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\nhttp://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### IBM AutoAI Auto-Generated Notebook v1.14.3\n\n**Note:** Notebook code generated using AutoAI will execute successfully. If code is modified or reordered,   \nthere is no guarantee it will successfully execute. This pipeline is optimized for the original dataset.  \nThe pipeline may fail or produce sub-optimium results if used with different data. For different data,  \nplease consider returning to AutoAI Experiments to generate a new pipeline. Please read our documentation   \nfor more information:   \n(Cloud Platform) https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n(Cloud Pak For Data) https://www.ibm.com/support/knowledgecenter/SSQNUZ_3.0.0/wsj/analyze-data/autoai-notebook.html .\n\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:   \nThe notebook converts dataframes to numpy arrays before fitting the pipeline   \n(a current restriction of the preprocessor pipeline). The known_values_list is passed by reference   \nand populated with categorical values during fit of the preprocessing pipeline. Delete its members before re-fitting."}, {"metadata": {}, "cell_type": "markdown", "source": "### Representing Pipeline_4 \n"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Set Up\nIf lightgbm or xgboost installation fails, please follow:\n - [lightgbm docs](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html)\n - [xgboost docs](https://xgboost.readthedocs.io/en/latest/build.html)"}, {"metadata": {}, "cell_type": "code", "source": "\ntry:\n    import autoai_libs\nexcept Exception as e:\n    import subprocess\n    out = subprocess.check_output('pip install autoai-libs'.split(' '))\n    for line in out.splitlines():\n        print(line)\n    import autoai_libs\nimport sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import inf, nan, dtype, mean\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "b'Collecting autoai-libs'\nb'  Downloading autoai_libs-1.12.4-48-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)'\nb'Requirement already satisfied: numpy>=1.16.4 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from autoai-libs) (1.18.5)'\nb'Collecting pandas<1.0.0,>=0.24.2'\nb'  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)'\nb'Requirement already satisfied: scikit-learn<0.23.2,>=0.20.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from autoai-libs) (0.23.1)'\nb'Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas<1.0.0,>=0.24.2->autoai-libs) (2.8.1)'\nb'Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas<1.0.0,>=0.24.2->autoai-libs) (2020.1)'\nb'Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn<0.23.2,>=0.20.3->autoai-libs) (1.5.0)'\nb'Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn<0.23.2,>=0.20.3->autoai-libs) (0.16.0)'\nb'Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn<0.23.2,>=0.20.3->autoai-libs) (2.1.0)'\nb'Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas<1.0.0,>=0.24.2->autoai-libs) (1.15.0)'\nb'Installing collected packages: pandas, autoai-libs'\nb'  Attempting uninstall: pandas'\nb'    Found existing installation: pandas 1.0.5'\nb'    Uninstalling pandas-1.0.5:'\nb'      Successfully uninstalled pandas-1.0.5'\nb'Successfully installed autoai-libs-1.12.4 pandas-0.25.3'\nlightgbm, if needed, will be installed and imported later\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# compose a decorator to assist pipeline instantiation via import of modules and installation of packages\ndef decorator_retries(func):\n    def install_import_retry(*args, **kwargs):\n        retries = 0\n        successful = False\n        failed_retries = 0\n        while retries < 100 and failed_retries < 10 and not successful:\n            retries += 1\n            failed_retries += 1\n            try:\n                result = func(*args, **kwargs)\n                successful = True\n            except Exception as e:\n                estr = str(e)\n                if estr.startswith('name ') and estr.endswith(' is not defined'):\n                    try:\n                        import importlib\n                        module_name = estr.split(\"'\")[1]\n                        module = importlib.import_module(module_name)\n                        globals().update({module_name: module})\n                        print('import successful for ' + module_name)\n                        failed_retries -= 1\n                    except Exception as import_failure:\n                        print('import of ' + module_name + ' failed with: ' + str(import_failure))\n                        import subprocess\n                        if module_name == 'lightgbm':\n                            try:\n                                print('attempting pip install of ' + module_name)\n                                process = subprocess.Popen('pip install ' + module_name, shell=True)\n                                process.wait()\n                            except Exception as E:\n                                print(E)\n                                try:\n                                    import sys\n                                    print('attempting conda install of ' + module_name)\n                                    process = subprocess.Popen('conda install --yes --prefix {sys.prefix} -c powerai ' + module_name, shell = True)\n                                    process.wait()\n                                except Exception as lightgbm_installation_error:\n                                    print('lightgbm installation failed!' + lightgbm_installation_error)\n                        else:\n                            print('attempting pip install of ' + module_name)\n                            process = subprocess.Popen('pip install ' + module_name, shell=True)\n                            process.wait()\n                        try:\n                            print('re-attempting import of ' + module_name)\n                            module = importlib.import_module(module_name)\n                            globals().update({module_name: module})\n                            print('import successful for ' + module_name)\n                            failed_retries -= 1\n                        except Exception as import_or_installation_failure:\n                            print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n                                import_or_installation_failure))\n                            raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n                                                       '? Try import and/or pip install manually?'))\n                elif type(e) is AttributeError:\n                    if 'module ' in estr and ' has no attribute ' in estr:\n                        pieces = estr.split(\"'\")\n                        if len(pieces) == 5:\n                            try:\n                                import importlib\n                                print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n                                module = importlib.import_module('.' + pieces[3], pieces[1])\n                                failed_retries -= 1\n                            except:\n                                print('failed attempt to import ' + pieces[3])\n                                raise (e)\n                        else:\n                            raise (e)\n                else:\n                    raise (e)\n        if successful:\n            print('Pipeline successfully instantiated')\n        else:\n            raise (ModuleNotFoundError(\n                'Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n        return result\n    return install_import_retry\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2. Compose Pipeline"}, {"metadata": {}, "cell_type": "code", "source": "# metadata necessary to replicate AutoAI scores with the pipeline\n_input_metadata = {'separator': ',', 'excel_sheet': 0, 'target_label_name': 'Purchased', 'learning_type': 'classification', 'subsampling': None, 'pos_label': 1, 'pn': 'P4', 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'optimization_metric': 'roc_auc', 'random_state': 33, 'data_source': ''}\n\n# define a function to compose the pipeline, and invoke it\n@decorator_retries\ndef compose_pipeline():\n    import numpy\n    from numpy import nan, dtype, mean\n    #\n    # composing steps for toplevel Pipeline\n    #\n    _input_metadata = {'separator': ',', 'excel_sheet': 0, 'target_label_name': 'Purchased', 'learning_type': 'classification', 'subsampling': None, 'pos_label': 1, 'pn': 'P4', 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'optimization_metric': 'roc_auc', 'random_state': 33, 'data_source': ''}\n    steps = []\n    steps.append(('column_selector', autoai_libs.transformers.exportable.ColumnSelector(activate_flag=True, columns_indices_list=[1, 2, 3])))\n    #\n    # composing steps for preprocessor Pipeline\n    #\n    preprocessor__input_metadata = None\n    preprocessor_steps = []\n    #\n    # composing steps for preprocessor_features FeatureUnion\n    #\n    preprocessor_features_transformer_list = []\n    #\n    # composing steps for preprocessor_features_categorical Pipeline\n    #\n    preprocessor_features_categorical__input_metadata = None\n    preprocessor_features_categorical_steps = []\n    preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0])))\n    preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash', dtypes_list=['char_str'], missing_values_reference_list=['', '-', '?', nan], misslist_list=[[]])))\n    preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n    preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan, filling_values_list=[nan], known_values_list=[[243382710732531489693195870365903964183, 132302907946296948163998721551176913917]], missing_values_reference_list=['', '-', '?', nan])))\n    preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n    preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan, sklearn_version_family='23', strategy='most_frequent')))\n    preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto', dtype=numpy.float64, encoding='ordinal', handle_unknown='error', sklearn_version_family='23')))\n    preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n    # assembling preprocessor_features_categorical_ Pipeline\n    preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n    preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n    #\n    # composing steps for preprocessor_features_numeric Pipeline\n    #\n    preprocessor_features_numeric__input_metadata = None\n    preprocessor_features_numeric_steps = []\n    preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[1, 2])))\n    preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True, dtypes_list=['int_num', 'int_num'], missing_values_reference_list=[])))\n    preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n    preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n    preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None, num_scaler_with_std=None, use_scaler_flag=False)))\n    preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n    # assembling preprocessor_features_numeric_ Pipeline\n    preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n    preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n    # assembling preprocessor_features_ FeatureUnion\n    preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n    preprocessor_steps.append(('features', preprocessor_features_pipeline))\n    preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0, permutation_indices=[0, 1, 2])))\n    # assembling preprocessor_ Pipeline\n    preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n    steps.append(('preprocessor', preprocessor_pipeline))\n    #\n    # composing steps for cognito Pipeline\n    #\n    cognito__input_metadata = None\n    cognito_steps = []\n    cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TAM(tans_class=sklearn.decomposition._pca.PCA(copy=True, iterated_power='auto', n_components=None, random_state=None, svd_solver='auto', tol=0.0, whiten=False), name='pca', tgraph=None, apply_all=True, col_names=['Gender', 'Age', 'EstimatedSalary'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n    cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 3), additional_col_count_to_keep=4, ptype='classification')))\n    cognito_steps.append(('2', autoai_libs.cognito.transforms.transform_utils.TA2(fun=numpy.add, name='sum', datatypes1=['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints1=[autoai_libs.utils.fc_methods.is_not_categorical], datatypes2=['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints2=[autoai_libs.utils.fc_methods.is_not_categorical], tgraph=None, apply_all=True, col_names=['Gender', 'Age', 'EstimatedSalary', 'pca_0', 'pca_1', 'pca_2'], col_dtypes=[dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects=None)))\n    cognito_steps.append(('3', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep=range(0, 3), additional_col_count_to_keep=4, ptype='classification')))\n    # assembling cognito_ Pipeline\n    cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n    steps.append(('cognito', cognito_pipeline))\n    steps.append(('estimator', xgboost.sklearn.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0.17553152807855085, learning_rate=0.02, max_delta_step=0, max_depth=6, min_child_weight=5, missing=None, n_estimators=243, n_jobs=1, nthread=None, objective='binary:logistic', random_state=33, reg_alpha=1.0, reg_lambda=0.9979209374840852, scale_pos_weight=1.3405979252567852, seed=None, silent=True, subsample=0.9971975206332722, verbosity=0, tree_method='hist')))\n    # assembling  Pipeline\n    pipeline = sklearn.pipeline.Pipeline(steps=steps)\n    return pipeline\npipeline = compose_pipeline()\n", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Pipeline successfully instantiated\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 3. Extract needed parameter values from AutoAI run metadata"}, {"metadata": {}, "cell_type": "code", "source": "\n# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\n#data_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\nif 'data_provenance' in _input_metadata:\n    data_provenance = _input_metadata['data_provenance']\nelse:\n    data_provenance = None\nif 'pos_label' in _input_metadata and learning_type == 'classification':\n    pos_label = _input_metadata['pos_label']\nelse:\n    pos_label = None\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4. Create dataframe from dataset in Cloud Object Storage"}, {"metadata": {}, "cell_type": "code", "source": "\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_0 = {\n    'ENDPOINT': 'https://s3.eu-geo.objectstorage.softlayer.net',\n    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token/',\n    'APIKEY': 'BDRJpmHIHszGAJ9xYwEi-VfLtzUW_OlvxA3L04HnMA2n',\n    'BUCKET': 'predictwhatproductscustomerslikel-donotdelete-pr-yncvuljc3cawgb',\n    'FILE': 'Social_Network_Ads.csv',\n    'SERVICE_NAME': 's3',\n    'ASSET_ID': '1',\n    }\n", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#  Read the data as a dataframe\nimport pandas as pd\n\ncsv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\ndf = None\nreadable = None  # if automatic detection fails, you can supply a filename here\n\n# First, obtain a readable object\n# Cloud Object Storage data access\n# Assumes COS credentials are in a dictionary named 'credentials_0'\n   \ncredentials = df = globals().get('credentials_0')       \nif readable is None and credentials is not None :\n    try:\n        import types\n        import pandas as pd\n        import io\n    except Exception as import_exception:\n        print('Error with importing packages - check if you installed them on your environment')\n    try:\n        if credentials['SERVICE_NAME'] == 's3':\n            try:\n                from botocore.client import Config\n                import ibm_boto3\n            except Exception as import_exception:\n                print('Installing required packages!')\n                !pip install ibm-cos-sdk\n                print('accessing data via Cloud Object Storage')\n            try:\n                client = ibm_boto3.client(service_name=credentials['SERVICE_NAME'],\n                                    ibm_api_key_id=credentials['APIKEY'],\n                                    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n                                    config=Config(signature_version='oauth'),\n                                    endpoint_url=credentials['ENDPOINT'])\n            except Exception as cos_exception:\n                print('unable to create client for cloud object storage')\n            try:\n                readable = client.get_object(Bucket=credentials['BUCKET'],Key=credentials['FILE'])['Body']\n                # add missing __iter__ method, so pandas accepts readable as file-like object\n                if not hasattr(readable, \"__iter__\"): readable.__iter__ = types.MethodType( __iter__, readable )\n            except Exception as cos_access_exception:\n                print('unable to access data object in cloud object storage with credentials supplied')\n        elif credentials['SERVICE_NAME'] == 'fs':\n            print('accessing data via File System')\n            try:\n                if credentials['FILE'].endswith('xlsx') or credentials['FILE'].endswith('xls'):\n                    df = pd.read_excel(credentials['FILE'])\n                else:\n                    df = pd.read_csv(credentials['FILE'], sep = _input_metadata['separator'])\n            except Exception as FS_access_exception:\n                print('unable to access data object in File System with path supplied') \n    except Exception as data_access_exception:\n        print('unable to access data object with credentials supplied') \n\n# IBM Cloud Pak for Data data access\nproject_filename = globals().get('project_filename')       \nif readable is None and 'credentials_0' in globals() and 'ASSET_ID' in credentials_0:\n    project_filename = credentials_0['ASSET_ID']\nif project_filename != None and project_filename != '1':\n    print('attempting project_lib access to ' + str(project_filename))\n    try:\n        from project_lib import Project\n        project = Project.access()\n        storage_credentials = project.get_storage_metadata()\n        readable = project.get_file(project_filename)\n    except Exception as project_exception:\n        print('unable to access data using the project_lib interface and filename supplied')\n\n# Use data_provenance as filename if other access mechanisms are unsuccessful\nif readable is None and type(data_provenance) is str:\n    print('attempting to access local file using path and name ' + data_provenance)\n    readable = data_provenance\n\n# Second, use pd.read_csv to read object, iterating over list of csv_encodings until successful\nif readable is not None:\n    for encoding in csv_encodings:\n        try:\n            if credentials['FILE'].endswith('xlsx') or credentials['FILE'].endswith('xls'):\n                buffer = io.BytesIO(readable.read())\n                buffer.seek(0)\n                df = pd.read_excel(buffer, encoding=encoding,sheet_name=_input_metadata['excel_sheet'])\n            else:\n                df = pd.read_csv(readable, encoding = encoding, sep = _input_metadata['separator'])\n            print('successfully loaded dataframe using encoding = ' + str(encoding))\n            break\n        except Exception as exception_dataread:\n            print('unable to read csv using encoding ' + str(encoding))\n            print('handled error was ' + str(exception_dataread))\n    if df is None:\n        print('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings))\n        print(f'Please use \\'insert to code\\' on data panel to load dataframe.')\n        raise(ValueError('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings)))\n\nif isinstance(df,pd.DataFrame):\n    print('Data loaded succesfully')\n    if _input_metadata.get('subsampling') is not None:\n        df = df.sample(frac=_input_metadata['subsampling'], random_state=_input_metadata['random_state']) if _input_metadata['subsampling'] <= 1.0 else df.sample(n=_input_metadata['subsampling'], random_state=_input_metadata['random_state'])\nelse:\n    print('Data cannot be loaded with credentials supplied, please provide DataFrame with training data.')", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "successfully loaded dataframe using encoding = UTF-8\nData loaded succesfully\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 5. Preprocess Data"}, {"metadata": {}, "cell_type": "code", "source": "# Drop rows whose target is not defined\ntarget = target_label_name # your target name here\nif learning_type == 'regression':\n    df[target] = pd.to_numeric(df[target], errors='coerce')\ndf.dropna('rows', how='any', subset=[target], inplace=True)\n", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# extract X and y\ndf_X = df.drop(columns=[target])\ndf_y = df[target]\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Detach preprocessing pipeline (which needs to see all training data)\npreprocessor_index = -1\npreprocessing_steps = [] \nfor i, step in enumerate(pipeline.steps):\n    preprocessing_steps.append(step)\n    if step[0]=='preprocessor':\n        preprocessor_index = i\n        break\n#if len(pipeline.steps) > preprocessor_index+1 and pipeline.steps[preprocessor_index + 1][0] == 'cognito':\n    #preprocessor_index += 1\n    #preprocessing_steps.append(pipeline.steps[preprocessor_index])\nif preprocessor_index >= 0:\n    preprocessing_pipeline = Pipeline(memory=pipeline.memory, steps=preprocessing_steps)\n    pipeline = Pipeline(steps=pipeline.steps[preprocessor_index+1:])", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Preprocess X\n# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\nknown_values_list.clear()  #  known_values_list is filled in by the preprocessing_pipeline if needed\npreprocessing_pipeline.fit(df_X.values, df_y.values)\nX_prep = preprocessing_pipeline.transform(df_X.values)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 6. Split data into Training and Holdout sets"}, {"metadata": {}, "cell_type": "code", "source": "# determine learning_type and perform holdout split (stratify conditionally)\nif learning_type is None:\n    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n    from sklearn.utils.multiclass import type_of_target\n    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n        learning_type = 'classification'\n    else:\n        learning_type = 'regression'\n    print('learning_type determined by type_of_target as:',learning_type)\nelse:\n    print('learning_type specified as:',learning_type)\n    \nfrom sklearn.model_selection import train_test_split\nif learning_type == 'classification':\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state)\n", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "learning_type specified as: classification\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### 7. Generate features via Feature Engineering pipeline"}, {"metadata": {}, "cell_type": "code", "source": "#Detach Feature Engineering pipeline if next, fit it, and transform the training data\nfe_pipeline = None\nif pipeline.steps[0][0] == 'cognito':\n    try:\n        fe_pipeline = Pipeline(steps=[pipeline.steps[0]])\n        X = fe_pipeline.fit_transform(X, y)\n        X_holdout = fe_pipeline.transform(X_holdout)\n        pipeline.steps = pipeline.steps[1:]\n    except IndexError:\n        try:\n            print('Trying to compose pipeline with some of cognito steps')\n            fe_pipeline = Pipeline(steps = list([pipeline.steps[0][1].steps[0],pipeline.steps[0][1].steps[1]]))\n            X = fe_pipeline.fit_transform(X, y)\n            X_holdout = fe_pipeline.transform(X_holdout)\n            pipeline.steps = pipeline.steps[1:]\n        except IndexError:\n            print('Composing pipeline without cognito steps!')\n            pipeline.steps = pipeline.steps[1:]\n", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": " ### 8. Additional setup: Define a function that returns a scorer for the target's positive label"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# create a function to produce a scorer for a given positive label\ndef make_pos_label_scorer(scorer, pos_label):\n    kwargs = {'pos_label':pos_label}\n    for prop in ['needs_proba', 'needs_threshold']:\n        if prop+'=True' in scorer._factory_args():\n            kwargs[prop] = True\n    if scorer._sign == -1:\n        kwargs['greater_is_better'] = False\n    from sklearn.metrics import make_scorer\n    scorer=make_scorer(scorer._score_func, **kwargs)\n    return scorer", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 9. Fit pipeline, predict on Holdout set, calculate score, perform cross-validation"}, {"metadata": {"pycharm": {"is_executing": false}}, "cell_type": "code", "source": "# fit the remainder of the pipeline on the training data\npipeline.fit(X,y)", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "Pipeline(steps=[('estimator',\n                 XGBClassifier(gamma=0.17553152807855085, learning_rate=0.02,\n                               max_depth=6, min_child_weight=5,\n                               n_estimators=243, random_state=33, reg_alpha=1.0,\n                               reg_lambda=0.9979209374840852,\n                               scale_pos_weight=1.3405979252567852, silent=True,\n                               subsample=0.9971975206332722, tree_method='hist',\n                               verbosity=0))])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# predict on the holdout data\ny_pred = pipeline.predict(X_holdout)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# compute score for the optimization metric\n# scorer may need pos_label, but not all scorers take pos_label parameter\nfrom sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscore = None\n#score = scorer(pipeline, X_holdout, y_holdout)  # this would suffice for simple cases\npos_label = None  # if you want to supply the pos_label, specify it here\nif pos_label is None and 'pos_label' in _input_metadata:\n    pos_label=_input_metadata['pos_label']\ntry:\n    score = scorer(pipeline, X_holdout, y_holdout)\nexcept Exception as e1:\n    if learning_type is \"classification\" and (pos_label is None or str(pos_label)==''):\n        print('You may have to provide a value for pos_label in order for a score to be calculated.')\n        raise(e1)\n    else:\n        exception_string=str(e1)\n        if 'pos_label' in exception_string:\n            try:\n                scorer = make_pos_label_scorer(scorer, pos_label=pos_label)\n                score = scorer(pipeline, X_holdout, y_holdout)\n                print('Retry was successful with pos_label supplied to scorer')\n            except Exception as e2:\n                print('Initial attempt to use scorer failed.  Exception was:')\n                print(e1)\n                print('')\n                print('Retry with pos_label failed.  Exception was:')\n                print(e2)\n        else:\n            raise(e1)\n\nif score is not None:\n    print(score)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "0.9532967032967032\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# cross_validate pipeline using training data\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import StratifiedKFold, KFold\nif learning_type == 'classification':\n    fold_generator = StratifiedKFold(n_splits=cv_num_folds, random_state=random_state)\nelse:\n    fold_generator = KFold(n_splits=cv_num_folds, random_state=random_state)\ncv_results = cross_validate(pipeline, X, y, cv=fold_generator, scoring={optimization_metric:scorer}, return_train_score=True)\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n  FutureWarning\n", "name": "stderr"}, {"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "0.9373301117487163"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "cv_results", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "{'fit_time': array([0.07001448, 0.06119418, 0.064641  ]),\n 'score_time': array([0.00305915, 0.00292373, 0.00283957]),\n 'test_roc_auc': array([0.9640592 , 0.90969496, 0.93823618]),\n 'train_roc_auc': array([0.97576261, 0.97938689, 0.97198732])}"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}